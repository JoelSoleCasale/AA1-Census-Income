{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "# Importing the models to be tested\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.covariance import EmpiricalCovariance\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from auxiliar_func import *\n",
    "from plot_func import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter tunning: cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model combinations to be tested: 48\n"
     ]
    }
   ],
   "source": [
    "target = 'income_50k'               # target variable\n",
    "df_tr = pd.read_csv('../train.csv') # training set\n",
    "\n",
    "TARGET_METRIC = 'f1_macro'          # metric to be used in the grid search\n",
    "SEED = 42                           # seed for reproducibility\n",
    "\n",
    "# Grid of preprocessing hyperparameters for each model\n",
    "prep_params_grid = {\n",
    "    'scaling': [None, 'minmax', 'standard'],\n",
    "    'imputation': ['mode'],\n",
    "    'cat_age': [False, True],\n",
    "    'merge_capital': [False, True],\n",
    "    'downsampling_method': ['random'],\n",
    "    'target_freq': [0.75, 0.8, 0.85, 0.9],\n",
    "    'generate_dummies': [True]\n",
    "}\n",
    "\n",
    "def n_comb(grid: dict, print_=True):\n",
    "    \"\"\"Returns the number of combinations to be tested given a grid of parameters\"\"\"\n",
    "    n = 1\n",
    "    for k in grid.keys():\n",
    "        n *= len(grid[k])\n",
    "    if print_:\n",
    "        print(f'Number of model combinations to be tested: {n}')\n",
    "    else:\n",
    "        return n\n",
    "\n",
    "def test_model(mod, prep_grid, mod_grid, name, rewrite=False, **kwargs):\n",
    "    \"\"\"Tests a model with all the possible combinations of preprocessing parameters and hyperparameters\"\"\"\n",
    "    if rewrite or not os.path.exists(f'./results/results_{name}.csv'):\n",
    "        try:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\") # to avoid convergence warnings\n",
    "                results = search_best_combination(mod, mod_grid, prep_grid, df_tr, target_metric=TARGET_METRIC, cv=5, N=15, **kwargs)\n",
    "                results.to_csv(f'./results/results_{name}.csv', index=False)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return None\n",
    "\n",
    "n_comb(prep_params_grid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model combinations to be tested: 5\n",
      "===Iteration 1===\n",
      "Searching preprocessing parameters...\n",
      "it: 17/48\r"
     ]
    }
   ],
   "source": [
    "mod = GaussianNB()\n",
    "\n",
    "mod_grid = {\n",
    "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]\n",
    "}\n",
    "\n",
    "n_comb(mod_grid)\n",
    "\n",
    "test_model(mod, prep_params_grid, mod_grid, 'nb', rewrite=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model combinations to be tested: 3\n"
     ]
    }
   ],
   "source": [
    "mod_par_grid = {\n",
    "    'solver': ['svd'], # 'svd' is faster and recommended for large datasets\n",
    "    'priors': [None], # By default, the class proportions are inferred from the training data.\n",
    "    'tol': [1e-4, 1e-3, 1e-2], # Absolute threshold for a singular value of X to be considered significant, used to estimate the rank of X.\n",
    "}\n",
    "\n",
    "n_comb(mod_par_grid) \n",
    "\n",
    "lda = LDA()\n",
    "test_model(lda, prep_params_grid, mod_par_grid, 'lda')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Discriminant Analysis (QDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model combinations to be tested: 36\n"
     ]
    }
   ],
   "source": [
    "mod_par_grid = {\n",
    "    'priors': [None], # By default, the class proportions are inferred from the training data.\n",
    "    'reg_param': [0.0, 0.1, 0.2, 0.3, 0.4, 0.5], # Regularizes the per-class covariance estimates: S = (1 - reg_param) * S + reg_param * np.eye(n_features),\n",
    "    'store_covariance': [True, False], # If True, the covariance matrices are computed and stored in the self.covariance_ attribute.\n",
    "    'tol': [1e-4, 1e-3, 1e-2], # Absolute threshold for a singular value of X to be considered significant, used to estimate the rank of X. Does not affect the predictions.\n",
    "}\n",
    "\n",
    "n_comb(mod_par_grid)\n",
    "\n",
    "qda = QDA()\n",
    "test_model(qda, prep_params_grid, mod_par_grid, 'qda')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model combinations to be tested: 14\n"
     ]
    }
   ],
   "source": [
    "mod_par_grid = {\n",
    "    'n_neighbors': [7, 9, 11, 13, 15, 17, 19], # Number of neighbors to use by default for kneighbors queries.\n",
    "    'weights': ['uniform', 'distance'], # weight function used in prediciton\n",
    "}\n",
    "\n",
    "n_comb(mod_par_grid)\n",
    "\n",
    "knn = KNN()\n",
    "test_model(knn, prep_params_grid, mod_par_grid, 'knn')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model combinations to be tested: 72\n"
     ]
    }
   ],
   "source": [
    "mod_par_grid = {\n",
    "    'penalty': ['l1', 'l2'], # Used to specify the norm used in the penalization.\n",
    "    'C': [0.1, 1, 10], # Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization.\n",
    "    'class_weight': [None, 'balanced'], \n",
    "    'fit_intercept': [True, False], # Specifies if a constant should be added to the decision function.\n",
    "    'intercept_scaling': [0.1, 1, 10],  \n",
    "    'max_iter': [1000], # Maximum number of iterations taken for the solvers to converge.\n",
    "    'multi_class': ['auto'],  \n",
    "    'random_state': [SEED],\n",
    "    'solver': ['saga'] # Algorithm to use in the optimization problem.\n",
    "}\n",
    "\n",
    "n_comb(mod_par_grid)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "test_model(logreg, prep_params_grid, mod_par_grid, 'logreg')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model combinations to be tested: 96\n"
     ]
    }
   ],
   "source": [
    "mod_par_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'loss': ['squared_hinge','hinge'],\n",
    "    'dual': [False],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'fit_intercept': [True, False],\n",
    "    'intercept_scaling': [0.1, 1],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'max_iter': [1000],\n",
    "    'random_state': [SEED]\n",
    "}\n",
    "\n",
    "n_comb(mod_par_grid)\n",
    "\n",
    "svm = LinearSVC()\n",
    "test_model(svm, prep_params_grid, mod_par_grid, 'svm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model combinations to be tested: 80\n"
     ]
    }
   ],
   "source": [
    "mod_par_grid = {\n",
    "    'n_estimators': [50, 65, 75, 100],\n",
    "    'criterion': ['gini'],\n",
    "    'max_depth': [None, 25, 30, 35, 40],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'random_state': [SEED],\n",
    "    'verbose': [0],\n",
    "    'warm_start': [False],\n",
    "    'class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "n_comb(mod_par_grid)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "test_model(rf, prep_params_grid, mod_par_grid, 'rf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting (XGBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model combinations to be tested: 72\n",
      "===Iteration 1===\n",
      "Searching preprocessing parameters...\n",
      "it: 48/48\n",
      "Searching model parameters...\n",
      "it: 72/72\n",
      "Best metric: 0.7904680227255335\n",
      "Best preprocessing parameters: [{'scaling': None, 'imputation': 'mode', 'cat_age': False, 'merge_capital': True, 'downsampling_method': 'random', 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': False, 'merge_capital': False, 'downsampling_method': 'random', 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': False, 'merge_capital': True, 'downsampling_method': 'random', 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': True, 'merge_capital': False, 'downsampling_method': 'random', 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': True, 'merge_capital': True, 'downsampling_method': 'random', 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': False, 'merge_capital': False, 'downsampling_method': 'random', 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'minmax', 'imputation': 'mode', 'cat_age': False, 'merge_capital': True, 'downsampling_method': 'random', 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': True, 'merge_capital': True, 'downsampling_method': 'random', 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'minmax', 'imputation': 'mode', 'cat_age': False, 'merge_capital': False, 'downsampling_method': 'random', 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'minmax', 'imputation': 'mode', 'cat_age': False, 'merge_capital': False, 'downsampling_method': 'random', 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': True, 'merge_capital': False, 'downsampling_method': 'random', 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'minmax', 'imputation': 'mode', 'cat_age': True, 'merge_capital': True, 'downsampling_method': 'random', 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'minmax', 'imputation': 'mode', 'cat_age': False, 'merge_capital': True, 'downsampling_method': 'random', 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'minmax', 'imputation': 'mode', 'cat_age': True, 'merge_capital': False, 'downsampling_method': 'random', 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'minmax', 'imputation': 'mode', 'cat_age': True, 'merge_capital': True, 'downsampling_method': 'random', 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}]\n",
      "Best model parameters: [{'n_estimators': 65, 'max_depth': None, 'learning_rate': 0.2, 'booster': 'gbtree'}, {'n_estimators': 75, 'max_depth': None, 'learning_rate': 0.2, 'booster': 'gbtree'}, {'n_estimators': 75, 'max_depth': None, 'learning_rate': 0.3, 'booster': 'gbtree'}, {'n_estimators': 50, 'max_depth': None, 'learning_rate': 0.2, 'booster': 'gbtree'}, {'n_estimators': 50, 'max_depth': None, 'learning_rate': 0.3, 'booster': 'gbtree'}, {'n_estimators': 65, 'max_depth': None, 'learning_rate': 0.3, 'booster': 'gbtree'}, {'n_estimators': 75, 'max_depth': None, 'learning_rate': 0.1, 'booster': 'gbtree'}, {'n_estimators': 65, 'max_depth': None, 'learning_rate': 0.1, 'booster': 'gbtree'}, {'n_estimators': 50, 'max_depth': None, 'learning_rate': 0.1, 'booster': 'gbtree'}, {'n_estimators': 50, 'max_depth': 25, 'learning_rate': 0.2, 'booster': 'gbtree'}, {'n_estimators': 65, 'max_depth': 25, 'learning_rate': 0.1, 'booster': 'gbtree'}, {'n_estimators': 75, 'max_depth': 30, 'learning_rate': 0.1, 'booster': 'gbtree'}, {'n_estimators': 65, 'max_depth': 35, 'learning_rate': 0.1, 'booster': 'gbtree'}, {'n_estimators': 50, 'max_depth': 30, 'learning_rate': 0.2, 'booster': 'gbtree'}, {'n_estimators': 65, 'max_depth': 30, 'learning_rate': 0.1, 'booster': 'gbtree'}]\n",
      "===Iteration 2===\n",
      "Searching preprocessing parameters...\n",
      "it: 14/14\n",
      "Searching model parameters...\n",
      "it: 13/13\n",
      "Best metric: 0.7948387654235688\n",
      "Best preprocessing parameters: [{'scaling': None, 'imputation': 'mode', 'cat_age': False, 'merge_capital': False, 'downsampling_method': 'random', 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': True, 'merge_capital': True, 'downsampling_method': 'random', 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': False, 'merge_capital': True, 'downsampling_method': 'random', 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': True, 'merge_capital': False, 'downsampling_method': 'random', 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'minmax', 'imputation': 'mode', 'cat_age': False, 'merge_capital': False, 'downsampling_method': 'random', 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': False, 'merge_capital': True, 'downsampling_method': 'random', 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': True, 'merge_capital': False, 'downsampling_method': 'random', 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': False, 'merge_capital': False, 'downsampling_method': 'random', 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'minmax', 'imputation': 'mode', 'cat_age': False, 'merge_capital': False, 'downsampling_method': 'random', 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'minmax', 'imputation': 'mode', 'cat_age': False, 'merge_capital': True, 'downsampling_method': 'random', 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': True, 'merge_capital': True, 'downsampling_method': 'random', 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'minmax', 'imputation': 'mode', 'cat_age': False, 'merge_capital': True, 'downsampling_method': 'random', 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'minmax', 'imputation': 'mode', 'cat_age': True, 'merge_capital': True, 'downsampling_method': 'random', 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'minmax', 'imputation': 'mode', 'cat_age': True, 'merge_capital': False, 'downsampling_method': 'random', 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'minmax', 'imputation': 'mode', 'cat_age': True, 'merge_capital': True, 'downsampling_method': 'random', 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}]\n",
      "Best model parameters: [{'n_estimators': 65, 'max_depth': None, 'learning_rate': 0.3, 'booster': 'gbtree'}, {'n_estimators': 75, 'max_depth': None, 'learning_rate': 0.3, 'booster': 'gbtree'}, {'n_estimators': 65, 'max_depth': None, 'learning_rate': 0.2, 'booster': 'gbtree'}, {'n_estimators': 75, 'max_depth': None, 'learning_rate': 0.2, 'booster': 'gbtree'}, {'n_estimators': 50, 'max_depth': None, 'learning_rate': 0.2, 'booster': 'gbtree'}, {'n_estimators': 50, 'max_depth': None, 'learning_rate': 0.3, 'booster': 'gbtree'}, {'n_estimators': 75, 'max_depth': None, 'learning_rate': 0.1, 'booster': 'gbtree'}, {'n_estimators': 75, 'max_depth': 30, 'learning_rate': 0.1, 'booster': 'gbtree'}, {'n_estimators': 65, 'max_depth': None, 'learning_rate': 0.1, 'booster': 'gbtree'}, {'n_estimators': 65, 'max_depth': 30, 'learning_rate': 0.1, 'booster': 'gbtree'}, {'n_estimators': 50, 'max_depth': 30, 'learning_rate': 0.2, 'booster': 'gbtree'}, {'n_estimators': 50, 'max_depth': 25, 'learning_rate': 0.2, 'booster': 'gbtree'}, {'n_estimators': 65, 'max_depth': 35, 'learning_rate': 0.1, 'booster': 'gbtree'}, {'n_estimators': 65, 'max_depth': 25, 'learning_rate': 0.1, 'booster': 'gbtree'}, {'n_estimators': 50, 'max_depth': None, 'learning_rate': 0.1, 'booster': 'gbtree'}]\n",
      "===Iteration 3===\n",
      "Searching preprocessing parameters...\n",
      "it: 13/13\n",
      "Searching model parameters...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod_par_grid = {\n",
    "    'n_estimators': [50, 65, 75],\n",
    "    'max_depth': [None, 25, 30, 35],\n",
    "    'learning_rate': [0.1, 0.2, 0.3],\n",
    "    'booster': ['gbtree', 'gblinear'],\n",
    "}\n",
    "\n",
    "n_comb(mod_par_grid)\n",
    "\n",
    "# for all categorical features, we need to remove special characters like '[', ']', and '<'\n",
    "df_tr = df_tr.applymap(lambda x: x.replace('[', '').replace(']', '').replace('<', '') if isinstance(x, str) else x)\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "test_model(xgb, prep_params_grid, mod_par_grid, 'xgb')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model combinations to be tested: 48\n"
     ]
    }
   ],
   "source": [
    "prep_params_grid3 = {\n",
    "    'scaling': [None],\n",
    "    'imputation': ['mode'],\n",
    "    'cat_age': [False],\n",
    "    'merge_capital': [False, True],\n",
    "    'downsampling_method': ['random'],\n",
    "    'target_freq': [0.8, 0.85, 0.9],\n",
    "    'generate_dummies': [False]\n",
    "}\n",
    "\n",
    "# a first preprocess to get the categorical features\n",
    "df_tr_pre = preprocessing(df_tr, imputation='mode', cat_age=False, generate_dummies=False)\n",
    "X_train, y_train = df_tr_pre.drop(target, axis=1), df_tr_pre[target]\n",
    "cat_features = list(X_train.select_dtypes(include=['category']).columns)\n",
    "\n",
    "mod_par_grid = {\n",
    "    'iterations': [500, 750],\n",
    "    'depth': [1, 2, 4, 6],\n",
    "    'border_count': [32, 64, 96],\n",
    "    'random_seed': [SEED],\n",
    "    'verbose': [0],\n",
    "    'loss_function': ['Logloss'],\n",
    "    'eval_metric': ['F1', 'AUC'],\n",
    "    'cat_features': [cat_features],\n",
    "}\n",
    "\n",
    "n_comb(mod_par_grid)\n",
    "\n",
    "cat_model = CatBoostClassifier()\n",
    "test_model(cat_model, prep_params_grid3, mod_par_grid, 'catboost')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
