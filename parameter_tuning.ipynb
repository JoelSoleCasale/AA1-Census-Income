{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "# Importing the models to be tested\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from auxiliar_func import *\n",
    "from plot_func import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model combinations to be tested: 72\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Census-Income-KDD.csv')\n",
    "target = 'income_50k'\n",
    "df_tr, df_te = train_test_split(df, test_size=0.3, random_state=42)\n",
    "\n",
    "TARGET_METRIC = 'f1_macro'\n",
    "SEED = 42\n",
    "\n",
    "prep_params_grid = {\n",
    "    'scaling': [None, 'minmax', 'standard'],\n",
    "    'imputation': ['mode'],\n",
    "    'cat_age': [False, True],\n",
    "    'merge_capital': [False, True],\n",
    "    'downsampling_method': ['random', 'NearMiss'],\n",
    "    'target_freq': [0.75, 0.8, 0.85],\n",
    "    'generate_dummies': [True]\n",
    "}\n",
    "\n",
    "def n_comb(grid: dict, print_=True):\n",
    "    n = 1\n",
    "    for k in grid.keys():\n",
    "        n *= len(grid[k])\n",
    "    if print_:\n",
    "        print(f'Number of model combinations to be tested: {n}')\n",
    "    else:\n",
    "        return n\n",
    "    \n",
    "n_comb(prep_params_grid)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameters for LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model combinations to be tested: 36\n",
      "===Iteration 1===\n",
      "Searching preprocessing parameters...\n",
      "it: 36/36\n",
      "Searching model parameters...\n",
      "it: 36/36\n",
      "Best metric: 0.757974545624374\n",
      "Best preprocessing parameters: [{'scaling': 'minmax', 'imputation': 'mode', 'cat_age': True, 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': True, 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'nancat', 'cat_age': True, 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'minmax', 'imputation': 'nancat', 'cat_age': True, 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'standard', 'imputation': 'mode', 'cat_age': True, 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'standard', 'imputation': 'nancat', 'cat_age': True, 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': False, 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'minmax', 'imputation': 'mode', 'cat_age': False, 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'nancat', 'cat_age': False, 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'minmax', 'imputation': 'nancat', 'cat_age': False, 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}]\n",
      "Best model parameters: [{'solver': 'svd', 'shrinkage': None, 'store_covariance': True, 'tol': 0.01}, {'solver': 'svd', 'shrinkage': None, 'store_covariance': False, 'tol': 0.01}, {'solver': 'svd', 'shrinkage': None, 'store_covariance': True, 'tol': 0.0001}, {'solver': 'svd', 'shrinkage': None, 'store_covariance': False, 'tol': 0.0001}, {'solver': 'svd', 'shrinkage': None, 'store_covariance': False, 'tol': 0.001}, {'solver': 'svd', 'shrinkage': None, 'store_covariance': True, 'tol': 0.001}, {'solver': 'lsqr', 'shrinkage': 'auto', 'store_covariance': False, 'tol': 0.0001}, {'solver': 'eigen', 'shrinkage': 'auto', 'store_covariance': False, 'tol': 0.001}, {'solver': 'eigen', 'shrinkage': 'auto', 'store_covariance': False, 'tol': 0.0001}, {'solver': 'eigen', 'shrinkage': 'auto', 'store_covariance': True, 'tol': 0.01}]\n",
      "===Iteration 2===\n",
      "Searching preprocessing parameters...\n",
      "it: 9/9\n",
      "Searching model parameters...\n",
      "it: 8/8\n"
     ]
    }
   ],
   "source": [
    "mod_par_grid = {\n",
    "    'solver': ['svd', 'lsqr', 'eigen'],\n",
    "    'shrinkage': [None, 'auto'],\n",
    "    'store_covariance': [True, False],\n",
    "    'tol': [1e-4, 1e-3, 1e-2],\n",
    "}\n",
    "\n",
    "n_comb(mod_par_grid)\n",
    "\n",
    "lda = LDA()\n",
    "\n",
    "if not os.path.exists('results_lda.csv'):\n",
    "    results = search_best_combination(lda, mod_par_grid, prep_params_grid, df_tr, target_metric=TARGET_METRIC)\n",
    "    results.to_csv('results_lda.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameters for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_par_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', None],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'fit_intercept': [True, False],\n",
    "    'intercept_scaling': [0.1, 1, 10],\n",
    "    'max_iter': [1000],\n",
    "    'multi_class': ['auto'],\n",
    "    'random_state': [SEED],\n",
    "    'solver': ['saga']\n",
    "}\n",
    "\n",
    "n_comb(mod_par_grid)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "if not os.path.exists('results_log_regression.csv'):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        results = search_best_combination(logreg, mod_par_grid, prep_params_grid, df_tr, target_metric=TARGET_METRIC)\n",
    "    results.to_csv('results_log_regression.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameters for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model combinations to be tested: 96\n",
      "===Iteration 1===\n",
      "Searching preprocessing parameters...\n",
      "it: 36/36\n",
      "Searching model parameters...\n",
      "it: 96/96\n",
      "Best metric: 0.6911005968409094\n",
      "Best preprocessing parameters: [{'scaling': None, 'imputation': 'nancat', 'cat_age': False, 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': False, 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': False, 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'nancat', 'cat_age': False, 'target_freq': 0.85, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'standard', 'imputation': 'mode', 'cat_age': False, 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}]\n",
      "Best model parameters: [{'penalty': 'l1', 'loss': 'squared_hinge', 'dual': False, 'C': 0.1, 'fit_intercept': True, 'intercept_scaling': 0.1, 'class_weight': None, 'max_iter': 1000, 'random_state': 42}, {'penalty': 'l1', 'loss': 'squared_hinge', 'dual': False, 'C': 0.1, 'fit_intercept': False, 'intercept_scaling': 0.1, 'class_weight': None, 'max_iter': 1000, 'random_state': 42}, {'penalty': 'l1', 'loss': 'squared_hinge', 'dual': False, 'C': 0.1, 'fit_intercept': False, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 1000, 'random_state': 42}, {'penalty': 'l2', 'loss': 'squared_hinge', 'dual': False, 'C': 0.1, 'fit_intercept': True, 'intercept_scaling': 0.1, 'class_weight': None, 'max_iter': 1000, 'random_state': 42}, {'penalty': 'l2', 'loss': 'squared_hinge', 'dual': False, 'C': 0.1, 'fit_intercept': True, 'intercept_scaling': 1, 'class_weight': None, 'max_iter': 1000, 'random_state': 42}]\n",
      "===Iteration 2===\n",
      "Searching preprocessing parameters...\n",
      "\n",
      "Searching model parameters...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod_par_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'loss': ['squared_hinge','hinge'],\n",
    "    'dual': [False],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'fit_intercept': [True, False],\n",
    "    'intercept_scaling': [0.1, 1],\n",
    "    'class_weight': [None, 'balanced'],\n",
    "    'max_iter': [1000],\n",
    "    'random_state': [SEED]\n",
    "}\n",
    "\n",
    "n_comb(mod_par_grid)\n",
    "\n",
    "svm = LinearSVC()\n",
    "\n",
    "if not os.path.exists('results_svm.csv'):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        results = search_best_combination(svm, mod_par_grid, prep_params_grid, df_tr, target_metric=TARGET_METRIC, verbose=2)\n",
    "    results.to_csv('results_svm.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameters for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model combinations to be tested: 144\n",
      "===Iteration 1===\n",
      "Searching preprocessing parameters...\n",
      "it: 36/36\n",
      "Searching model parameters...\n",
      "it: 144/144\n",
      "Best metric: 0.7074323534081814\n",
      "Best preprocessing parameters: [{'scaling': None, 'imputation': 'mode', 'cat_age': False, 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': False, 'target_freq': 0.75, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'standard', 'imputation': 'mode', 'cat_age': False, 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'minmax', 'imputation': 'mode', 'cat_age': False, 'target_freq': 0.75, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'minmax', 'imputation': 'mode', 'cat_age': False, 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}]\n",
      "Best model parameters: [{'n_estimators': 50, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'random_state': 42, 'verbose': 0, 'warm_start': False, 'class_weight': 'balanced'}, {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 30, 'max_features': 'sqrt', 'random_state': 42, 'verbose': 0, 'warm_start': False, 'class_weight': 'balanced'}, {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 20, 'max_features': 'sqrt', 'random_state': 42, 'verbose': 0, 'warm_start': False, 'class_weight': 'balanced'}, {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 25, 'max_features': 'sqrt', 'random_state': 42, 'verbose': 0, 'warm_start': False, 'class_weight': 'balanced'}, {'n_estimators': 100, 'criterion': 'entropy', 'max_depth': 25, 'max_features': 'log2', 'random_state': 42, 'verbose': 0, 'warm_start': False, 'class_weight': 'balanced_subsample'}]\n",
      "===Iteration 2===\n",
      "Searching preprocessing parameters...\n",
      "it: 4/4\n",
      "Searching model parameters...\n",
      "it: 4/4\n",
      "Best metric: 0.7146489324396472\n",
      "Best preprocessing parameters: [{'scaling': 'minmax', 'imputation': 'mode', 'cat_age': False, 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'standard', 'imputation': 'mode', 'cat_age': False, 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': False, 'target_freq': 0.75, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': 'minmax', 'imputation': 'mode', 'cat_age': False, 'target_freq': 0.75, 'generate_dummies': True, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': False, 'target_freq': 0.8, 'generate_dummies': True, 'remove_duplicates': True}]\n",
      "Best model parameters: [{'n_estimators': 50, 'criterion': 'gini', 'max_depth': 20, 'max_features': 'sqrt', 'random_state': 42, 'verbose': 0, 'warm_start': False, 'class_weight': 'balanced'}, {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 30, 'max_features': 'sqrt', 'random_state': 42, 'verbose': 0, 'warm_start': False, 'class_weight': 'balanced'}, {'n_estimators': 50, 'criterion': 'gini', 'max_depth': 25, 'max_features': 'sqrt', 'random_state': 42, 'verbose': 0, 'warm_start': False, 'class_weight': 'balanced'}, {'n_estimators': 100, 'criterion': 'entropy', 'max_depth': 25, 'max_features': 'log2', 'random_state': 42, 'verbose': 0, 'warm_start': False, 'class_weight': 'balanced_subsample'}, {'n_estimators': 50, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'random_state': 42, 'verbose': 0, 'warm_start': False, 'class_weight': 'balanced'}]\n",
      "===Iteration 3===\n",
      "Searching preprocessing parameters...\n",
      "it: 3/3\n",
      "Searching model parameters...\n",
      "it: 3/3\n"
     ]
    }
   ],
   "source": [
    "mod_par_grid = {\n",
    "    'n_estimators': [50, 75, 100],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 20, 25, 30],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'random_state': [SEED],\n",
    "    'verbose': [0],\n",
    "    'warm_start': [False],\n",
    "    'class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "}\n",
    "\n",
    "n_comb(mod_par_grid)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "if not os.path.exists('results_rf.csv'):\n",
    "    results = search_best_combination(rf, mod_par_grid, prep_params_grid, df_tr, target_metric=TARGET_METRIC)\n",
    "    results.to_csv('results_rf.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning parameters for catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of model combinations to be tested: 54\n",
      "===Iteration 1===\n",
      "Searching preprocessing parameters...\n",
      "it: 3/3\n",
      "Searching model parameters...\n",
      "it: 54/54\n",
      "Best metric: 0.6690903906697929\n",
      "Best preprocessing parameters: [{'scaling': None, 'imputation': 'mode', 'cat_age': False, 'target_freq': 0.8, 'generate_dummies': False, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': False, 'target_freq': 0.75, 'generate_dummies': False, 'remove_duplicates': True}, {'scaling': None, 'imputation': 'mode', 'cat_age': False, 'target_freq': 0.85, 'generate_dummies': False, 'remove_duplicates': True}]\n",
      "Best model parameters: [{'iterations': 5, 'depth': 6, 'border_count': 128, 'random_seed': 42, 'verbose': 0, 'loss_function': 'Logloss', 'eval_metric': 'AUC', 'class_weights': [1, 1], 'cat_features': ['class_worker', 'det_ind_code', 'det_occ_code', 'education', 'hs_college', 'marital_stat', 'major_ind_code', 'major_occ_code', 'race', 'hisp_origin', 'sex', 'union_member', 'unemp_reason', 'full_or_part_emp', 'tax_filer_stat', 'region_prev_res', 'state_prev_res', 'det_hh_fam_stat', 'det_hh_summ', 'mig_same', 'fam_under_18', 'country_father', 'country_mother', 'country_self', 'citizenship', 'own_or_self', 'vet_question', 'vet_benefits', 'year']}, {'iterations': 5, 'depth': 6, 'border_count': 128, 'random_seed': 42, 'verbose': 0, 'loss_function': 'Logloss', 'eval_metric': 'F1', 'class_weights': [1, 1], 'cat_features': ['class_worker', 'det_ind_code', 'det_occ_code', 'education', 'hs_college', 'marital_stat', 'major_ind_code', 'major_occ_code', 'race', 'hisp_origin', 'sex', 'union_member', 'unemp_reason', 'full_or_part_emp', 'tax_filer_stat', 'region_prev_res', 'state_prev_res', 'det_hh_fam_stat', 'det_hh_summ', 'mig_same', 'fam_under_18', 'country_father', 'country_mother', 'country_self', 'citizenship', 'own_or_self', 'vet_question', 'vet_benefits', 'year']}, {'iterations': 5, 'depth': 8, 'border_count': 64, 'random_seed': 42, 'verbose': 0, 'loss_function': 'Logloss', 'eval_metric': 'AUC', 'class_weights': [1, 2], 'cat_features': ['class_worker', 'det_ind_code', 'det_occ_code', 'education', 'hs_college', 'marital_stat', 'major_ind_code', 'major_occ_code', 'race', 'hisp_origin', 'sex', 'union_member', 'unemp_reason', 'full_or_part_emp', 'tax_filer_stat', 'region_prev_res', 'state_prev_res', 'det_hh_fam_stat', 'det_hh_summ', 'mig_same', 'fam_under_18', 'country_father', 'country_mother', 'country_self', 'citizenship', 'own_or_self', 'vet_question', 'vet_benefits', 'year']}, {'iterations': 5, 'depth': 8, 'border_count': 64, 'random_seed': 42, 'verbose': 0, 'loss_function': 'Logloss', 'eval_metric': 'F1', 'class_weights': [1, 2], 'cat_features': ['class_worker', 'det_ind_code', 'det_occ_code', 'education', 'hs_college', 'marital_stat', 'major_ind_code', 'major_occ_code', 'race', 'hisp_origin', 'sex', 'union_member', 'unemp_reason', 'full_or_part_emp', 'tax_filer_stat', 'region_prev_res', 'state_prev_res', 'det_hh_fam_stat', 'det_hh_summ', 'mig_same', 'fam_under_18', 'country_father', 'country_mother', 'country_self', 'citizenship', 'own_or_self', 'vet_question', 'vet_benefits', 'year']}, {'iterations': 5, 'depth': 8, 'border_count': 32, 'random_seed': 42, 'verbose': 0, 'loss_function': 'Logloss', 'eval_metric': 'AUC', 'class_weights': [1, 1], 'cat_features': ['class_worker', 'det_ind_code', 'det_occ_code', 'education', 'hs_college', 'marital_stat', 'major_ind_code', 'major_occ_code', 'race', 'hisp_origin', 'sex', 'union_member', 'unemp_reason', 'full_or_part_emp', 'tax_filer_stat', 'region_prev_res', 'state_prev_res', 'det_hh_fam_stat', 'det_hh_summ', 'mig_same', 'fam_under_18', 'country_father', 'country_mother', 'country_self', 'citizenship', 'own_or_self', 'vet_question', 'vet_benefits', 'year']}]\n",
      "===Iteration 2===\n",
      "Searching preprocessing parameters...\n",
      "it: 2/2\n",
      "Searching model parameters...\n",
      "it: 4/4\n"
     ]
    }
   ],
   "source": [
    "prep_params_grid2 = {\n",
    "    'scaling': [None],\n",
    "    'imputation': ['mode'],\n",
    "    'cat_age': [False],\n",
    "    'target_freq': [0.75, 0.8, 0.85],\n",
    "    'generate_dummies': [False]\n",
    "}\n",
    "\n",
    "# a first preprocess to get the categorical features\n",
    "df_tr_pre = preprocessing(df_tr, imputation='mode', cat_age=False, generate_dummies=False)\n",
    "X_train, y_train = df_tr_pre.drop(target, axis=1), df_tr_pre[target]\n",
    "cat_features = list(X_train.select_dtypes(include=['category']).columns)\n",
    "\n",
    "mod_par_grid = {\n",
    "    'iterations': [5],\n",
    "    'depth': [6, 8, 10],\n",
    "    'border_count': [32, 64, 128],\n",
    "    'random_seed': [SEED],\n",
    "    'verbose': [0],\n",
    "    'loss_function': ['Logloss'],\n",
    "    'eval_metric': ['F1', 'AUC'],\n",
    "    'class_weights': [[1, 1], [1, 2], [1, 3]],\n",
    "    'cat_features': [cat_features],\n",
    "}\n",
    "\n",
    "n_comb(mod_par_grid)\n",
    "\n",
    "cat_model = CatBoostClassifier()\n",
    "\n",
    "if not os.path.exists('results_catboost.csv'):\n",
    "    results = search_best_combination(cat_model, mod_par_grid, prep_params_grid2, df_tr, target_metric=TARGET_METRIC)\n",
    "    results.to_csv('results_catboost.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aa1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
